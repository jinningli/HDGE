{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ruipenghan/projects/research/ssbrl/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.special import softmax\n",
    "import os\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "def get_cls_embedding(tweet):\n",
    "    #model.save_pretrained(MODEL)\n",
    "    # text = \"Covid cases are increasing fast! Covid cases are increasing fast! \"\n",
    "    # text = preprocess(text)\n",
    "    encoded_input = tokenizer(tweet, return_tensors='pt')\n",
    "\n",
    "    output = model(**encoded_input, output_hidden_states=True)\n",
    "    # print(\"output:\")\n",
    "    # print(len(output.hidden_states))\n",
    "    # print(output.hidden_states[-1])\n",
    "\n",
    "    last_layer_hidden_states = output.hidden_states[-1]\n",
    "    # Option 1: Use the embedding of the [CLS] token (index 0)\n",
    "    cls_embedding = last_layer_hidden_states[:, 0, :]\n",
    "    # print(cls_embedding.shape)\n",
    "    # print(cls_embedding)\n",
    "    return cls_embedding.detach().numpy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import os\n",
    "# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "csv_path = \"/Users/ruipenghan/projects/research/ssbrl/data/paper_data_merged/EDCA_filtered_data_10_5_5_20000_tree_vis_edca_labeled_merged.csv\"\n",
    "data = pd.read_csv(csv_path)\n",
    "\n",
    "data = data.drop_duplicates(subset=\"index_text\", keep=\"first\")\n",
    "data = data.drop_duplicates(subset=\"text\", keep=\"first\")\n",
    "tweets = data[\"text\"].tolist()\n",
    "index_text = data[\"index_text\"].tolist()\n",
    "assert(len(tweets) == len(index_text))\n",
    "embeddings = []\n",
    "i = 0\n",
    "for tweet in tweets:\n",
    "    if i == 5:\n",
    "        break\n",
    "    embeddings.append(get_cls_embedding(tweet))\n",
    "    i += 1\n",
    "\n",
    "\n",
    "# RUN KMEANS on cls_embedding\n",
    "# Convert the list of embeddings to a numpy array\n",
    "embeddings_array = np.array(embeddings)\n",
    "\n",
    "# Reshape (5, 1, 768) to (5, 768)\n",
    "embeddings_array = np.squeeze(embeddings_array)\n",
    "\n",
    "# Create an instance of KMeans with 2 clusters\n",
    "kmeans = KMeans(n_clusters=2)\n",
    "\n",
    "# Fit the KMeans model to the embeddings\n",
    "kmeans.fit(embeddings_array)\n",
    "\n",
    "# Get the cluster labels for each embedding\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Print the cluster labels\n",
    "# print(cluster_labels)\n",
    "clustered_data = pd.read_csv(csv_path)\n",
    "for i, label in enumerate(cluster_labels):\n",
    "    km_label = \"opposing\"\n",
    "    if label == 0:\n",
    "        km_label = \"supportive\"\n",
    "    idx_text = index_text[i]\n",
    "    # print((clustered_data['index_text'] == idx_text).sum())\n",
    "    # exit(1)\n",
    "    clustered_data.loc[clustered_data['index_text'] == idx_text, 'gpt_label'] = km_label\n",
    "\n",
    "folder_path = \"labelled/edca\"\n",
    "\n",
    "# Create the folder if it doesn't exist\n",
    "if not os.path.exists(folder_path):\n",
    "    os.makedirs(folder_path)\n",
    "\n",
    "# Define the file path\n",
    "file_path = os.path.join(folder_path, \"clustered_data.csv\")\n",
    "\n",
    "# Save the dataframe as a CSV file\n",
    "clustered_data.to_csv(file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores = output[0][0].detach().numpy()\n",
    "# scores = softmax(scores)\n",
    "\n",
    "# ranking = np.argsort(scores)\n",
    "# ranking = ranking[::-1]\n",
    "# for i in range(scores.shape[0]):\n",
    "#     l = config.id2label[ranking[i]]\n",
    "#     s = scores[ranking[i]]\n",
    "#     print(f\"{i+1}) {l} {np.round(float(s), 4)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
